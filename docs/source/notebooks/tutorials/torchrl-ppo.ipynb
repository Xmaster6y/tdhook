{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9177ccf8",
   "metadata": {},
   "source": [
    "# TorchRL PPO Action Probing\n",
    "\n",
    "This notebook demonstrates how to set up a TorchRL PPO agent and use tdhook to probe action representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49cb2df",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9e492ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "\n",
    "DEV = True\n",
    "\n",
    "if importlib.util.find_spec(\"google.colab\") is not None:\n",
    "    MODE = \"colab-dev\" if DEV else \"colab\"\n",
    "else:\n",
    "    MODE = \"local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e8d6330",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"colab\":\n",
    "    %pip install -q tdhook torchrl\n",
    "elif MODE == \"colab-dev\":\n",
    "    !rm -rf tdhook\n",
    "    !git clone https://github.com/Xmaster6y/tdhook -b main\n",
    "    %pip install -q ./tdhook torchrl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72561bf",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bb1fdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchrl.envs import TransformedEnv, Compose, DoubleToFloat, StepCounter\n",
    "from torchrl.envs.libs.gym import GymEnv\n",
    "from torchrl.modules import MLP, ProbabilisticActor, NormalParamExtractor, TanhNormal\n",
    "from torchrl.objectives import ClipPPOLoss\n",
    "from torchrl.objectives.value import GAE\n",
    "from tensordict.nn import TensorDictModule\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from tdhook.latent.probing import Probing, ProbeManager\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79909a3e",
   "metadata": {},
   "source": [
    "## Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b7ef980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Composite(\n",
      "    observation: UnboundedContinuous(\n",
      "        shape=torch.Size([11]),\n",
      "        space=ContinuousBox(\n",
      "            low=Tensor(shape=torch.Size([11]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "            high=Tensor(shape=torch.Size([11]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "        device=cpu,\n",
      "        dtype=torch.float32,\n",
      "        domain=continuous),\n",
      "    step_count: BoundedDiscrete(\n",
      "        shape=torch.Size([1]),\n",
      "        space=ContinuousBox(\n",
      "            low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, contiguous=True),\n",
      "            high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, contiguous=True)),\n",
      "        device=cpu,\n",
      "        dtype=torch.int64,\n",
      "        domain=discrete),\n",
      "    device=None,\n",
      "    shape=torch.Size([]),\n",
      "    data_cls=None)\n",
      "Action space: BoundedContinuous(\n",
      "    shape=torch.Size([1]),\n",
      "    space=ContinuousBox(\n",
      "        low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "        high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "    device=cpu,\n",
      "    dtype=torch.float32,\n",
      "    domain=continuous)\n"
     ]
    }
   ],
   "source": [
    "env_name = \"InvertedDoublePendulum-v4\"\n",
    "base_env = GymEnv(env_name)\n",
    "env = TransformedEnv(\n",
    "    base_env,\n",
    "    Compose(\n",
    "        DoubleToFloat(),\n",
    "        StepCounter(),\n",
    "    ),\n",
    ")\n",
    "print(f\"Observation space: {env.observation_spec}\")\n",
    "print(f\"Action space: {env.action_spec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f49d050",
   "metadata": {},
   "source": [
    "## Create Actor and Critic Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40af70ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor network created\n",
      "Critic network created\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 32\n",
    "num_cells = 6\n",
    "\n",
    "# Actor network\n",
    "actor_module = TensorDictModule(\n",
    "    nn.Sequential(\n",
    "        MLP(\n",
    "            in_features=env.observation_spec[\"observation\"].shape[-1],\n",
    "            out_features=2 * env.action_spec.shape[-1],\n",
    "            num_cells=[hidden_size] * num_cells,\n",
    "        ),\n",
    "        NormalParamExtractor(),\n",
    "    ),\n",
    "    in_keys=[\"observation\"],\n",
    "    out_keys=[\"loc\", \"scale\"],\n",
    ")\n",
    "\n",
    "actor = ProbabilisticActor(\n",
    "    module=actor_module,\n",
    "    spec=env.action_spec,\n",
    "    in_keys=[\"loc\", \"scale\"],\n",
    "    distribution_class=TanhNormal,\n",
    "    distribution_kwargs={\n",
    "        \"low\": env.action_spec.space.low,\n",
    "        \"high\": env.action_spec.space.high,\n",
    "    },\n",
    "    return_log_prob=True,\n",
    ")\n",
    "\n",
    "# Critic network\n",
    "critic = TensorDictModule(\n",
    "    MLP(\n",
    "        in_features=env.observation_spec[\"observation\"].shape[-1],\n",
    "        out_features=1,\n",
    "        num_cells=[hidden_size] * num_cells,\n",
    "    ),\n",
    "    in_keys=[\"observation\"],\n",
    "    out_keys=[\"state_value\"],\n",
    ")\n",
    "\n",
    "print(\"Actor network created\")\n",
    "print(\"Critic network created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9224dd7",
   "metadata": {},
   "source": [
    "## Create PPO Loss Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffd81e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO loss module created\n"
     ]
    }
   ],
   "source": [
    "advantage_module = GAE(\n",
    "    gamma=0.99,\n",
    "    lmbda=0.95,\n",
    "    value_network=critic,\n",
    "    average_gae=True,\n",
    "    device=device,\n",
    "    deactivate_vmap=True,\n",
    ")\n",
    "\n",
    "loss_module = ClipPPOLoss(\n",
    "    actor_network=actor,\n",
    "    critic_network=critic,\n",
    "    clip_epsilon=0.2,\n",
    "    entropy_bonus=True,\n",
    "    entropy_coeff=1e-4,\n",
    "    critic_coeff=1.0,\n",
    "    loss_critic_type=\"smooth_l1\",\n",
    "    functional=False,\n",
    ")\n",
    "\n",
    "print(\"PPO loss module created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80626dab",
   "metadata": {},
   "source": [
    "## Collect Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "917be032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([100])\n",
      "Batch keys: _TensorDictKeysView(['step_count', 'action', 'observation', 'done', 'terminated', 'truncated', ('next', 'observation'), ('next', 'step_count'), ('next', 'reward'), ('next', 'done'), ('next', 'terminated'), ('next', 'truncated'), ('next', 'state_value'), ('collector', 'traj_ids'), 'loc', 'scale', 'action_log_prob', 'state_value', 'advantage', 'value_target'],\n",
      "    include_nested=True,\n",
      "    leaves_only=True)\n"
     ]
    }
   ],
   "source": [
    "from torchrl.collectors import SyncDataCollector\n",
    "\n",
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    actor,\n",
    "    frames_per_batch=100,\n",
    "    total_frames=100,\n",
    ")\n",
    "\n",
    "# Collect a batch of data\n",
    "batch = next(iter(collector))\n",
    "advantage_module(batch)\n",
    "\n",
    "print(f\"Batch shape: {batch.shape}\")\n",
    "print(f\"Batch keys: {batch.keys(True, True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ebc555",
   "metadata": {},
   "source": [
    "## Set Up Action Probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2bf814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "indices = torch.randperm(batch.numel())\n",
    "split_idx = int(0.8 * batch.numel())\n",
    "train_indices, test_indices = indices[:split_idx], indices[split_idx:]\n",
    "train_batch = batch[train_indices]\n",
    "test_batch = batch[test_indices]\n",
    "\n",
    "# Create probe manager\n",
    "probe_manager = ProbeManager(\n",
    "    LinearRegression,\n",
    "    {},\n",
    "    lambda preds, labels: {\"r2\": r2_score(labels, preds)},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533a263d",
   "metadata": {},
   "source": [
    "## Run Probing on Actor and Critic Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b47fc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook into actor and critic layers to probe action representations\n",
    "with Probing(\n",
    "    \"td_module.(critic_network.module.\\d+|actor_network.module.0.module.0.\\d+)\",\n",
    "    probe_manager.probe_factory,\n",
    "    additional_keys=[\"labels\", \"step_type\"],\n",
    "    relative=False,\n",
    ").prepare(loss_module) as hooked_module:\n",
    "    # Fit probes on training data\n",
    "    train_batch[\"labels\"] = train_batch[\"action\"]\n",
    "    train_batch[\"step_type\"] = \"fit\"\n",
    "    hooked_module(train_batch)\n",
    "\n",
    "    # Evaluate probes on test data\n",
    "    test_batch[\"labels\"] = test_batch[\"action\"]\n",
    "    test_batch[\"step_type\"] = \"predict\"\n",
    "    hooked_module(test_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43323201",
   "metadata": {},
   "source": [
    "## Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87dbeee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R² scores:\n",
      "  td_module.actor_network.module.0.module.0.0_fwd: 0.057\n",
      "  td_module.actor_network.module.0.module.0.1_fwd: 0.457\n",
      "  td_module.actor_network.module.0.module.0.2_fwd: 0.457\n",
      "  td_module.actor_network.module.0.module.0.3_fwd: 0.424\n",
      "  td_module.actor_network.module.0.module.0.4_fwd: 0.424\n",
      "  td_module.actor_network.module.0.module.0.5_fwd: 0.392\n",
      "  td_module.actor_network.module.0.module.0.6_fwd: 0.391\n",
      "  td_module.actor_network.module.0.module.0.7_fwd: 0.475\n",
      "  td_module.actor_network.module.0.module.0.8_fwd: 0.429\n",
      "  td_module.actor_network.module.0.module.0.9_fwd: 0.399\n",
      "  td_module.actor_network.module.0.module.0.10_fwd: 0.376\n",
      "  td_module.actor_network.module.0.module.0.11_fwd: 0.377\n",
      "  td_module.actor_network.module.0.module.0.12_fwd: 0.003\n",
      "  td_module.critic_network.module.0_fwd: 0.057\n",
      "  td_module.critic_network.module.1_fwd: 0.380\n",
      "  td_module.critic_network.module.2_fwd: 0.380\n",
      "  td_module.critic_network.module.3_fwd: 0.315\n",
      "  td_module.critic_network.module.4_fwd: 0.314\n",
      "  td_module.critic_network.module.5_fwd: 0.318\n",
      "  td_module.critic_network.module.6_fwd: 0.316\n",
      "  td_module.critic_network.module.7_fwd: 0.333\n",
      "  td_module.critic_network.module.8_fwd: 0.307\n",
      "  td_module.critic_network.module.9_fwd: 0.307\n",
      "  td_module.critic_network.module.10_fwd: 0.288\n",
      "  td_module.critic_network.module.11_fwd: 0.337\n",
      "  td_module.critic_network.module.12_fwd: 0.013\n",
      "\n",
      "Test R² scores:\n",
      "  td_module.actor_network.module.0.module.0.0_fwd: 0.160\n",
      "  td_module.actor_network.module.0.module.0.1_fwd: -0.743\n",
      "  td_module.actor_network.module.0.module.0.2_fwd: -0.743\n",
      "  td_module.actor_network.module.0.module.0.3_fwd: -2.509\n",
      "  td_module.actor_network.module.0.module.0.4_fwd: -2.506\n",
      "  td_module.actor_network.module.0.module.0.5_fwd: -2.913\n",
      "  td_module.actor_network.module.0.module.0.6_fwd: -2.848\n",
      "  td_module.actor_network.module.0.module.0.7_fwd: -2.626\n",
      "  td_module.actor_network.module.0.module.0.8_fwd: -3.136\n",
      "  td_module.actor_network.module.0.module.0.9_fwd: -2.513\n",
      "  td_module.actor_network.module.0.module.0.10_fwd: -2.363\n",
      "  td_module.actor_network.module.0.module.0.11_fwd: -1.602\n",
      "  td_module.actor_network.module.0.module.0.12_fwd: 0.038\n",
      "  td_module.critic_network.module.0_fwd: 0.160\n",
      "  td_module.critic_network.module.1_fwd: -2.264\n",
      "  td_module.critic_network.module.2_fwd: -2.264\n",
      "  td_module.critic_network.module.3_fwd: -1.375\n",
      "  td_module.critic_network.module.4_fwd: -1.235\n",
      "  td_module.critic_network.module.5_fwd: -1.553\n",
      "  td_module.critic_network.module.6_fwd: -1.480\n",
      "  td_module.critic_network.module.7_fwd: -0.289\n",
      "  td_module.critic_network.module.8_fwd: -0.219\n",
      "  td_module.critic_network.module.9_fwd: -0.883\n",
      "  td_module.critic_network.module.10_fwd: -0.278\n",
      "  td_module.critic_network.module.11_fwd: -1.791\n",
      "  td_module.critic_network.module.12_fwd: 0.120\n"
     ]
    }
   ],
   "source": [
    "print(\"Training R² scores:\")\n",
    "for key, value in probe_manager.fit_metrics.items():\n",
    "    print(f\"  {key}: {value['r2']:.3f}\")\n",
    "\n",
    "print(\"\\nTest R² scores:\")\n",
    "for key, value in probe_manager.predict_metrics.items():\n",
    "    print(f\"  {key}: {value['r2']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9f79b6",
   "metadata": {},
   "source": [
    "**Note:** The R² scores shown above are expected to be poor (often negative) because the model is not trained. The actor and critic networks are initialized with random weights, so their internal representations do not yet encode meaningful information about actions. After training the PPO agent, you would expect to see higher R² scores, indicating that the network layers learn to represent action-relevant information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
